output_dir: ${oc.env:LARGE_DATA_PATH}/huggingface/transformers

data:
  hf_dataset_identifier: "helena-balabin/vg_coco_overlap_for_graphormer"
  hf_dataset_identifier_processed: "helena-balabin/vg_coco_overlap_for_graphormer_processed"
  image_base_path: ${oc.env:LARGE_DATA_PATH}/vg/coco_images
  push_to_hub: false
  split: "train"
  seed: 42
  num_proc: 8
  batch_size: 256
  validation_split: 0.1
  n_samples: -1 # -1 for all

model:
  pretrained_model_name_or_path: "openai/clip-vit-base-patch32"
  huggingface_hub_model_id: "helena-balabin/clip-graphormer"
  model_type: "image"
  model_type_graph_base: "filtered"
  graphormer_size: "small"

training:
  batch_size: 128
  gradient_accumulation_steps: 4
  learning_rate: 2e-5
  epochs: 25 # 1 epoch for testing, 25 for full training
  weight_decay: 0.01
  logging_steps: 50
  eval_steps: 100 # 1 step for testing, 100 for full training
  save_total_limit: 3

mlflow:
  tracking_uri: ${oc.env:MLFLOW_TRACKING_URI}
  experiment_name: "GraphCLIP + Image Graphs Training"
